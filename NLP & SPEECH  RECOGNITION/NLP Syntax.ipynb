{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COURSE:   PGP [AI&ML]\n",
    "\n",
    "## Learner :  Chaitanya Kumar Battula\n",
    "## Module  : NLP\n",
    "## Topic   : NLP_Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them.',\n",
       " 'Thanks.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "s.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Tokenize  from a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just', 'setting', 'up', 'my', 'twttr', 'tm', 'help', 'Wondering', 'if', 'I']\n"
     ]
    }
   ],
   "source": [
    "filename = 'tweets1.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just', 'setting', 'up', 'my', 'twttr', 'tm', 'help', 'Wondering', 'if', 'I']\n"
     ]
    }
   ],
   "source": [
    "# load text\n",
    "filename = 'tweets1.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split based on words only\n",
    "import re\n",
    "words = re.split(r'\\W+', text)\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by Whitespace and Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just', 'setting', 'up', 'my', 'twttr', 'tm', 'help', 'Wondering', 'if', 'I']\n"
     ]
    }
   ],
   "source": [
    "# load text\n",
    "filename = 'tweets1.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in words]\n",
    "print(stripped[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m', 'r', '.', 'a', 'j', 'a', 'y', ' ', 'k', 'u', 'm', 'a', 'r', ' ', 'b']\n"
     ]
    }
   ],
   "source": [
    "filename = 'tweets1.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into words by white space\n",
    "words = \"Mr.Ajay Kumar B\"\n",
    "# convert to lower case\n",
    "words = [word.lower() for word in words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just setting up my twttr\\ntm help\\nWondering if I should quit work and become a monk who secretly writes serial murder thriller novels.', 'Going to bed.', 'gary coleman sighting by Zoom.']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "filename =  'tweets1.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into sentences\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just', 'setting', 'up']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "filename =  'tweets1.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "\n",
    "# split into sentences\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "sentences = word_tokenize(text)\n",
    "print(sentences[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Out Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just', 'setting', 'up', 'my', 'twttr', 'tm', 'help', 'Wondering', 'if', 'I', 'should', 'quit', 'work', 'and', 'become', 'a', 'monk', 'who', 'secretly', 'writes', 'serial', 'murder', 'thriller', 'novels', 'Going', 'to', 'bed', 'gary', 'coleman', 'sighting', 'by', 'Zoom', 'The', 'rumors', 'were', 'true', 'Festmob', 'In', 'The', 'Signal', 'premiere', 'Lotsa', 'buyers', 'Good', 'luck', 'gang', 'listening', 'to', 'the', 'pocast', 'net', 'nite', 'while', 'setting', 'up', 'twitter', 'watching', 'windows', 'promo', 'video', 'lol', 'getting', 'ready', 'for', 'work', 'going', 'to', 'work', 'in', 'vista', 'training', 'this', 'is', 'boring', 'just', 'came', 'back', 'from', 'supper', 'now', 'to', 'go', 'back', 'to', 'boring', 'vista', 'training', 'just', 'finished', 'my', 'last', 'break', 'now', 'for', 'an', 'office', 'module', 'i', 'have', 'to']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "filename =  'tweets1.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  View Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Filter Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setting', 'twttr', 'tm', 'help', 'Wondering', 'I', 'quit', 'work', 'become', 'monk', 'secretly', 'writes', 'serial', 'murder', 'thriller', 'novels', 'Going', 'bed', 'gary', 'coleman', 'sighting', 'Zoom', 'The', 'rumors', 'true', 'Festmob', 'In', 'The', 'Signal', 'premiere', 'Lotsa', 'buyers', 'Good', 'luck', 'gang', 'listening', 'pocast', 'net', 'nite', 'setting', 'twitter', 'watching', 'windows', 'promo', 'video', 'lol', 'getting', 'ready', 'work', 'going', 'work', 'vista', 'training', 'boring', 'came', 'back', 'supper', 'go', 'back', 'boring', 'vista', 'training', 'finished', 'last', 'break', 'office', 'module', 'get', 'hours', 'drive', 'sister', 'going', 'catch', 'finished', 'office', 'test', 'got', 'vista', 'training', 'yeah', 'another', 'day', 'dry', 'boring', 'half', 'us', 'sites', 'lol', 'work', 'dell', 'know', 'much', 'microsoft', 'take', 'jsut', 'finished', 'vista', 'training', 'yeah', 'Just']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "filename =  'tweets1.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just', 'set', 'up', 'my', 'twttr', 'tm', 'help', 'wonder', 'if', 'I', 'should', 'quit', 'work', 'and', 'becom', 'a', 'monk', 'who', 'secretli', 'write', 'serial', 'murder', 'thriller', 'novel', '.', 'go', 'to', 'bed', '.', 'gari', 'coleman', 'sight', 'by', 'zoom', '.', 'the', 'rumor', 'were', 'true', 'festmob', '.', 'In', 'the', 'signal', 'premier', '.', 'lotsa', 'buyer', '.', 'good', 'luck', 'gang', 'listen', 'to', 'the', 'pocast', 'net', '@', 'nite', 'while', 'set', 'up', 'twitter', 'watch', 'window', '386', 'promo', 'video', ',', 'lol', 'get', 'readi', 'for', 'work', 'go', 'to', 'work', 'in', 'vista', 'train', ',', 'thi', 'is', 'bore', 'just', 'came', 'back', 'from', 'supper', ',', 'now', 'to', 'go', 'back', 'to', 'bore', 'vista', 'train', 'just', 'finish']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "filename =  'tweets1.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# stemming of words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(stemmed[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PipeLine Demo\n",
    "\n",
    "Load the raw text.\n",
    "Split into tokens.\n",
    "Convert to lowercase.\n",
    "Remove punctuation from each token.\n",
    "Filter out remaining tokens that are not alphabetic.\n",
    "Filter out tokens that are stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setting', 'twttr', 'tm', 'help', 'wondering', 'quit', 'work', 'become', 'monk', 'secretly', 'writes', 'serial', 'murder', 'thriller', 'novels', 'going', 'bed', 'gary', 'coleman', 'sighting', 'zoom', 'rumors', 'true', 'festmob', 'signal', 'premiere', 'lotsa', 'buyers', 'good', 'luck', 'gang', 'listening', 'pocast', 'net', 'nite', 'setting', 'twitter', 'watching', 'windows', 'promo', 'video', 'lol', 'getting', 'ready', 'work', 'going', 'work', 'vista', 'training', 'boring', 'came', 'back', 'supper', 'go', 'back', 'boring', 'vista', 'training', 'finished', 'last', 'break', 'office', 'module', 'get', 'hours', 'drive', 'sister', 'going', 'catch', 'zs', 'finished', 'office', 'test', 'got', 'vista', 'training', 'yeah', 'another', 'day', 'dry', 'boring', 'half', 'us', 'sites', 'lol', 'work', 'dell', 'know', 'much', 'microsoft', 'take', 'jsut', 'finished', 'vista', 'training', 'yeah', 'set', 'phone', 'connection', 'twitter']\n",
      "\n",
      "After Stemming:\n",
      "['just', 'set', 'up', 'my', 'twttr', 'tm', 'help', 'wonder', 'if', 'i', 'should', 'quit', 'work', 'and', 'becom', 'a', 'monk', 'who', 'secretli', 'write', 'serial', 'murder', 'thriller', 'novel', '.', 'go', 'to', 'bed', '.', 'gari', 'coleman', 'sight', 'by', 'zoom', '.', 'the', 'rumor', 'were', 'true', 'festmob', '.', 'in', 'the', 'signal', 'premier', '.', 'lotsa', 'buyer', '.', 'good', 'luck', 'gang', 'listen', 'to', 'the', 'pocast', 'net', '@', 'nite', 'while', 'set', 'up', 'twitter', 'watch', 'window', '386', 'promo', 'video', ',', 'lol', 'get', 'readi', 'for', 'work', 'go', 'to', 'work', 'in', 'vista', 'train', ',', 'thi', 'is', 'bore', 'just', 'came', 'back', 'from', 'supper', ',', 'now', 'to', 'go', 'back', 'to', 'bore', 'vista', 'train', 'just', 'finish']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "filename =  'tweets1.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:100])\n",
    "\n",
    "print()\n",
    "# stemming of words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(\"After Stemming:\")\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happier: happy\n"
     ]
    }
   ],
   "source": [
    "#Loading the NLTK package for lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#creating the Object for Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#Printing the lemma of word happier\n",
    "print(\"happier:\", \n",
    "lemmatizer.lemmatize(\"happier\", pos=\"a\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helping : help\n"
     ]
    }
   ],
   "source": [
    "#Loading the nltk package to have PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "#create the object for PorterStemmer\n",
    "PS = PorterStemmer()\n",
    "#Print the stemmer of word helping\n",
    "print(\"helping :\",PS.stem(\"helping\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting all letters to lower or upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text in lowercase : python is a programming language that is interpreted and high\u0002level language\n"
     ]
    }
   ],
   "source": [
    "#Identify the user input\n",
    "user_input = \"Python is a Programming Language that is Interpreted and High\u0002Level language\"\n",
    "#Print the sentence by converting the text from uppercase to lowercase\n",
    "print(\"Text in lowercase :\", user_input.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting numbers into words or removing numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove numbers : Team A has  batsman and  bowlers, while team b has  batsman and  bowlers\n"
     ]
    }
   ],
   "source": [
    "#Loading the regex package to find number\n",
    "import re\n",
    "#identify user input\n",
    "input_str = \"Team A has 6 batsman and 5 bowlers, while team b has 5 batsman and 6 bowlers\"\n",
    "#remove numbers by using regex\n",
    "output = re.sub(r\"\\d+\", \"\", input_str)\n",
    "#print the sentence after removal of numbers\n",
    "print(\"remove numbers :\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing accent, punctuations marks, and other diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result after removing punctuation : Sentence having string with Punctuation\n"
     ]
    }
   ],
   "source": [
    "#Load the regex package and string package\n",
    "import re, string\n",
    "\n",
    "#define user input\n",
    "input_str = \"Sentence. having. string with. Punctuation?\"\n",
    "\n",
    "#remove punctuation\n",
    "result = re.sub('[%s]' % re.escape(string.punctuation), '', input_str)\n",
    "\n",
    "#print the sentence after removal of punctuation\n",
    "print(\"result after removing punctuation :\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing white spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove spaces using regex :pythonisprogramminglanguageHello\n",
      "\n",
      "Remove landing spaces using regex :pythonis programming language \t\n",
      "\r",
      "\tHello \t\n",
      "\n",
      "Remove trailing spaces using regex :pythonis programming language \t\n",
      "\r",
      "\tHello\n",
      "\n",
      "Remove landing spaces using regex :pythonis programming language \t\n",
      "\r",
      "\tHello\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the regex and string package\n",
    "import re\n",
    "\n",
    "#define input from user\n",
    "input_str = 'pythonis programming language \\t\\n\\r\\tHello \\t'\n",
    "\n",
    "#Print the sentence after removing the spaces\n",
    "print('Remove spaces using regex :', re.sub(r\"\\s+\", \"\", input_str),\"\\n\",sep='')\n",
    "\n",
    "#Print the sentence after removing the landing spaces\n",
    "print('Remove landing spaces using regex :', re.sub(r\"^\\s+\", \"\", input_str),\"\\n\", sep='')\n",
    "\n",
    "#Print the sentence after removing the trailing spaces\n",
    "print('Remove trailing spaces using regex :', re.sub(r\"\\s+$\", \"\", input_str),\"\\n\", sep='')\n",
    "\n",
    "#Print the sentence after removing the leading and trailing spaces\n",
    "print('Remove landing spaces using regex :', re.sub(r\"^\\s+|\\s+$\", \"\", input_str),\"\\n\", sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove stopwords : ['Stop', 'words', 'words', 'filtered', 'processing', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "#Load the stopwords package\n",
    "from nltk.corpus import stopwords\n",
    "#Load the word tokenizer package\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#define the user input\n",
    "input_str = \"Stop words are the words that are filtered before  and after processing of text.\"\n",
    "\n",
    "#crete object for stopwords\n",
    "stop_word = set(stopwords.words(\"english\"))\n",
    "\n",
    "#convert word into tokens\n",
    "token = word_tokenize(input_str)\n",
    "\n",
    "#remove stopwords from the list of tokens\n",
    "output = [i for i in token if not i in stop_word]\n",
    "\n",
    "#remove the stopwords and print the sentence\n",
    "print(\"remove stopwords :\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after removing stopwords : ['Tokenization is the way of tokenizing or dividing  a string, text into a list of tokens']\n"
     ]
    }
   ],
   "source": [
    "#Load the package for tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#defining user input\n",
    "text = \"Tokenization is the way of tokenizing or dividing  a string, text into a list of tokens\"\n",
    "\n",
    "#print the tokens of sentence.\n",
    "print(\"after removing stopwords :\", sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 'want', 'to')\n",
      "('want', 'to', 'ngramize')\n",
      "('to', 'ngramize', 'the')\n",
      "('ngramize', 'the', 'foo')\n",
      "('the', 'foo', 'bar')\n",
      "('foo', 'bar', 'sentences')\n"
     ]
    }
   ],
   "source": [
    "#Load the package for ngrams\n",
    "from nltk import ngrams\n",
    "\n",
    "#define user input\n",
    "usr_input = 'i want to ngramize the foo bar sentences'\n",
    "\n",
    "#define number of gram\n",
    "n = 3\n",
    "\n",
    "#split the sentence to make grams\n",
    "sixgrams = ngrams(usr_input.split(), n)\n",
    "for grams in sixgrams:\n",
    "    #Print the 3 grams of user-input\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

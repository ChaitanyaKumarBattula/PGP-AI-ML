{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COURSE:   PGP [AI&ML]\n",
    "\n",
    "## Learner :  Chaitanya Kumar Battula\n",
    "## Module  : NLP\n",
    "## Topic   : NLTK Basics Tutorial-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To download or Update nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download()\n",
    "# Will display a graphical interface for downloading NLTL will be presented:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Moby Dick by Herman Melville 1851>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1: Call me Ishmael .\n",
      "sent2: The family of Dashwood had long been settled in Sussex .\n",
      "sent3: In the beginning God created the heaven and the earth .\n",
      "sent4: Fellow - Citizens of the Senate and of the House of Representatives :\n",
      "sent5: I have a problem with people PMing me to lol JOIN\n",
      "sent6: SCENE 1 : [ wind ] [ clop clop clop ] KING ARTHUR : Whoa there !\n",
      "sent7: Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "sent8: 25 SEXY MALE , seeks attrac older single lady , for discreet encounters .\n",
      "sent9: THE suburb of Saffron Park lay on the sunset side of London , as red and ragged as a cloud of sunset .\n"
     ]
    }
   ],
   "source": [
    "sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Call', 'me', 'Ishmael', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK corpus is a massive dump of all kinds of natural language data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available corpus names:\n",
      "['_LazyModule__lazymodule_globals', '_LazyModule__lazymodule_import', '_LazyModule__lazymodule_init', '_LazyModule__lazymodule_loaded', '_LazyModule__lazymodule_locals', '_LazyModule__lazymodule_name', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__']\n"
     ]
    }
   ],
   "source": [
    "import nltk.corpus\n",
    "dir(nltk.corpus)\n",
    "print(\"\\nAvailable corpus names:\")\n",
    "print(dir(nltk.corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brown Corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on. 1.1 gives an example of each genre (for a complete list, see http://icame.uib.no/brown/bcm-los.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Too', 'often', 'a', 'beginning', 'bodybuilder', ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories='hobbies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reuters Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Reuters Corpus contains 10,788 news documents totaling 1.3 million words. The documents have been classified into 90 topics, and grouped into two sets, called \"training\" and \"test\"; thus, the text with fileid 'test/14826' is a document drawn from the test set. This split is for training and testing algorithms that automatically detect the topic of a document, as we will see in chap-data-intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/14826', 'test/14828', 'test/14829', 'test/14832', 'test/14833']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "f = reuters.fileids()\n",
    "f[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acq',\n",
       " 'alum',\n",
       " 'barley',\n",
       " 'bop',\n",
       " 'carcass',\n",
       " 'castor-oil',\n",
       " 'cocoa',\n",
       " 'coconut',\n",
       " 'coconut-oil',\n",
       " 'coffee',\n",
       " 'copper',\n",
       " 'copra-cake',\n",
       " 'corn',\n",
       " 'cotton',\n",
       " 'cotton-oil',\n",
       " 'cpi',\n",
       " 'cpu',\n",
       " 'crude',\n",
       " 'dfl',\n",
       " 'dlr',\n",
       " 'dmk',\n",
       " 'earn',\n",
       " 'fuel',\n",
       " 'gas',\n",
       " 'gnp',\n",
       " 'gold',\n",
       " 'grain',\n",
       " 'groundnut',\n",
       " 'groundnut-oil',\n",
       " 'heat',\n",
       " 'hog',\n",
       " 'housing',\n",
       " 'income',\n",
       " 'instal-debt',\n",
       " 'interest',\n",
       " 'ipi',\n",
       " 'iron-steel',\n",
       " 'jet',\n",
       " 'jobs',\n",
       " 'l-cattle',\n",
       " 'lead',\n",
       " 'lei',\n",
       " 'lin-oil',\n",
       " 'livestock',\n",
       " 'lumber',\n",
       " 'meal-feed',\n",
       " 'money-fx',\n",
       " 'money-supply',\n",
       " 'naphtha',\n",
       " 'nat-gas',\n",
       " 'nickel',\n",
       " 'nkr',\n",
       " 'nzdlr',\n",
       " 'oat',\n",
       " 'oilseed',\n",
       " 'orange',\n",
       " 'palladium',\n",
       " 'palm-oil',\n",
       " 'palmkernel',\n",
       " 'pet-chem',\n",
       " 'platinum',\n",
       " 'potato',\n",
       " 'propane',\n",
       " 'rand',\n",
       " 'rape-oil',\n",
       " 'rapeseed',\n",
       " 'reserves',\n",
       " 'retail',\n",
       " 'rice',\n",
       " 'rubber',\n",
       " 'rye',\n",
       " 'ship',\n",
       " 'silver',\n",
       " 'sorghum',\n",
       " 'soy-meal',\n",
       " 'soy-oil',\n",
       " 'soybean',\n",
       " 'strategic-metal',\n",
       " 'sugar',\n",
       " 'sun-meal',\n",
       " 'sun-oil',\n",
       " 'sunseed',\n",
       " 'tea',\n",
       " 'tin',\n",
       " 'trade',\n",
       " 'veg-oil',\n",
       " 'wheat',\n",
       " 'wpi',\n",
       " 'yen',\n",
       " 'zinc']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.nltk.org/book/ch02.html#:~:text=The%20Brown%20Corpus%20was%20the,in%201961%20at%20Brown%20University.&text=We%20can%20access%20the%20corpus,%3E%3E%3E%20from%20nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"All work and no play makes jack a dull boy, all work and no play\"\n",
    "Tokens = sent_tokenize(text)\n",
    "\n",
    "print(word_tokenize(Tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', ',', 'all', 'work', 'and', 'no', 'play']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import  word_tokenize\n",
    "\n",
    "text = \"All work and no play makes jack a dull boy, all work and no play\"\n",
    "Tokens = word_tokenize(text)\n",
    "print(Tokens )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web and Chat Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firefox.txt Cookie Manager: \"Don't allow sites that set removed cookies to se ...\n",
      "grail.txt SCENE 1: [wind] [clop clop clop] \n",
      "KING ARTHUR: Whoa there!  [clop ...\n",
      "overheard.txt White guy: So, do you have any plans for this evening?\n",
      "Asian girl ...\n",
      "pirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terr ...\n",
      "singles.txt 25 SEXY MALE, seeks attrac older single lady, for discreet encoun ...\n",
      "wine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawb ...\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import webtext\n",
    "\n",
    "for fileid in webtext.fileids():\n",
    "    print(fileid, webtext.raw(fileid)[:65], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inaugural Address Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the corpus is actually a collection of 55 texts, one for each presidential inagural address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt',\n",
       " '2013-Obama.txt',\n",
       " '2017-Trump.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from nltk.corpus import inaugural\n",
    "\n",
    "inaugural.fileids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['17',\n",
       " '17',\n",
       " '17',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '20',\n",
       " '20',\n",
       " '20',\n",
       " '20',\n",
       " '20']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[fileid[:4] for fileid in inaugural.fileids()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['17',\n",
       " '17',\n",
       " '17',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '18',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '19',\n",
       " '20',\n",
       " '20',\n",
       " '20',\n",
       " '20',\n",
       " '20']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[fileid[:2] for fileid in inaugural.fileids()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to check how the words America and citizen are used over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1b0069af730>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cfd = nltk.ConditionalFreqDist( (target, fileid[:4])\n",
    "              for fileid in inaugural.fileids()\n",
    "              for w in inaugural.words(fileid)\n",
    "              for target in ['america', 'citizen']\n",
    "              if w.lower().startswith(target)) [1]\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['learn', 'php', 'from', 'guru99', 'and', 'make', 'study', 'easy']\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text = \"learn php from guru99 and make study easy\"\n",
    "text_split = text.split()\n",
    "\n",
    "print(\"After Split:\", text_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God', 'is', 'Great', '!', 'I', 'won', 'a', 'lottery', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

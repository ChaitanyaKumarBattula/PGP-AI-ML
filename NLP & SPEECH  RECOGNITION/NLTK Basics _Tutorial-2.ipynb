{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COURSE:   PGP [AI&ML]\n",
    "\n",
    "## Learner :  Chaitanya Kumar Battula\n",
    "## Module  : NLP\n",
    "## Topic   : NLTK Basics Tutorial-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a text file  &  Read the Lines\n",
    "\n",
    "filename = 'FP.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open a text file and Read lines in a List\n",
    "\n",
    "filename = 'FP.txt'\n",
    "file = open(filename)\n",
    "List_Of_Lines = file.readlines()\n",
    "\n",
    "file.close()\n",
    "\n",
    "\n",
    "#List_Of_Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone.', 'Welcome to Canada.', 'You are studying NLP article']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence Tokenize  with NLTK\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "  \n",
    "text = \"Hello everyone. Welcome to Canada. You are studying NLP article\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " '.',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Canada',\n",
       " '.',\n",
       " 'You',\n",
       " 'are',\n",
       " 'studying',\n",
       " 'NLP',\n",
       " 'article']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Tokenize with word_tokenize :--\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "text = \"Hello everyone. Welcome to Canada. You are studying NLP article\"\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone.',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Canada.',\n",
       " 'You',\n",
       " 'are',\n",
       " 'studying',\n",
       " 'NLP',\n",
       " 'article']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenize with TreebankWordTokenizer:--\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "text = \"Hello everyone. Welcome to Canada. You are studying NLP article\"\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word  Tokenize with Using RegEx: RegexpTokenizer\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Canada',\n",
       " 'You',\n",
       " 'are',\n",
       " 'studying',\n",
       " 'NLP',\n",
       " 'article']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "text = \"Hello everyone. Welcome to Canada. You are studying NLP article\"\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word  Tokenize with Using RegEx: regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'everyone',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Canada',\n",
       " 'You',\n",
       " 'are',\n",
       " 'studying',\n",
       " 'NLP',\n",
       " 'article']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "  \n",
    "\n",
    "text = \"Hello everyone. Welcome to Canada. You are studying NLP article\"\n",
    "\n",
    "tokens = regexp_tokenize(text, \"[\\w']+\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stop Words from Word Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove stopwords : ['Stop', 'words', 'words', 'filtered', 'processing', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "#Load the stopwords package\n",
    "from nltk.corpus import stopwords\n",
    "#Load the word tokenizer package\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "#define the user input\n",
    "input_str = \"\"\" Stop words are the words that are filtered before  and after processing of text.\"\"\"\n",
    "\n",
    "#crete object for stopwords\n",
    "stop_word = set(stopwords.words(\"english\"))\n",
    "\n",
    "#convert word into tokens\n",
    "tokens = word_tokenize(input_str)\n",
    "\n",
    "#remove stopwords from the list of tokens\n",
    "output = [word for word in tokens if not word in stop_word]\n",
    "\n",
    "#remove the stopwords and print the sentence\n",
    "print(\"remove stopwords :\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming word -   with PortStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n",
      " stop words are the words that are filtered before  and after processing of text.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "#create an object of class PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "\n",
    "#proide a word to be stemmed\n",
    "\n",
    "print(porter.stem(\"cats\"))\n",
    "print(porter.stem(\"trouble\"))\n",
    "print(porter.stem(\"troubling\"))\n",
    "print(porter.stem(\"troubled\"))\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Stemming Words - with LancasterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "troubl\n",
      "troubl\n",
      "troubl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "#create an object of class PorterStemmer\n",
    "lancaster=LancasterStemmer()\n",
    "\n",
    "print(lancaster.stem(\"cats\"))\n",
    "print(lancaster.stem(\"trouble\"))\n",
    "print(lancaster.stem(\"troubling\"))\n",
    "print(lancaster.stem(\"troubled\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming a List of words - with Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "jump\n",
      "friend\n",
      "friendship\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "#create an object of class PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "#A list of words to be stemmed\n",
    "word_list = [\"cats\", \"jumping\", \"friends\", \"friendships\"]\n",
    "\n",
    "for word in word_list:\n",
    "    print(porter.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming a List of words - Lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "jump\n",
      "friend\n",
      "friend\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "#create an object of class PorterStemmer\n",
    "lancaster=LancasterStemmer()\n",
    "\n",
    "#A list of words to be stemmed\n",
    "word_list = [\"cats\", \"jumping\", \"friends\", \"friendships\"]\n",
    "\n",
    "for word in word_list:\n",
    "    print(lancaster.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between Porter and stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "\n",
    "\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Stemming a sentence to  a List Of stemmed  words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    tokens=word_tokenize(sentence)\n",
    "    stemmed_words=[]\n",
    "    for word in tokens:\n",
    "        stemmed_words.append(porter.stem(word))\n",
    "        \n",
    "        \n",
    "    return( stemmed_words)\n",
    "\n",
    "\n",
    "\n",
    "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
    "\n",
    "x=stemSentence(sentence)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Stemming a sentence to a stemmed sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python are veri intellig and work veri pythonli and now they are python their way to success . \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    tokens=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in tokens:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "        \n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "\n",
    "\n",
    "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
    "\n",
    "x=stemSentence(sentence)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snow ball  stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "englishStemmer=SnowballStemmer(\"english\")\n",
    "englishStemmer.stem(\"having\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'having'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "Stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "Stemmer.stem(\"having\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Did not understand the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python?utm_source=adwords_ppc&utm_campaignid=1455363063&utm_adgroupid=65083631748&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=278443377095&utm_targetid=aud-392016246653:dsa-429603003980&utm_loc_interest_ms=&utm_loc_physical_ms=9303585&gclid=EAIaIQobChMIseGhy6uc8QIVmp1LBR3HRgYTEAAYASAAEgLFWfD_BwE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercise:  Read a file, stemm and  save it as a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter=PorterStemmer()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stemmed_word = porter.stem(word)\n",
    "        print(stemmed_word)\n",
    "        stem_sentence.append(stemmed_word)\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "\n",
    "stem_file=open(\"FP1.txt\",mode=\"a+\", encoding=\"utf-8\")\n",
    "List_Of_Lines = stem_file.readlines()\n",
    "\n",
    "for line in List_Of_Lines:\n",
    "    stem_sentence=stemSentence(line)\n",
    "    stem_file.write(stem_sentence)\n",
    "\n",
    "stem_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code needs to be fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize a List Of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kites ---> kite\n",
      "babies ---> baby\n",
      "dogs ---> dog\n",
      "flying ---> flying\n",
      "smiling ---> smiling\n",
      "driving ---> driving\n",
      "died ---> died\n",
      "tried ---> tried\n",
      "feet ---> foot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "# Create WordNetLemmatizer object\n",
    "wnl = WordNetLemmatizer()\n",
    "  \n",
    "# single word lemmatization examples\n",
    "list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling', 'driving', 'died', 'tried', 'feet']\n",
    "for words in list1:\n",
    "    print(words + \" ---> \" + wnl.lemmatize(words))\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatiz  a sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'is', 'sitting', 'with', 'the', 'bats', 'on', 'the', 'striped', 'mat', 'under', 'many', 'flying', 'geese']\n",
      "the cat is sitting with the bat on the striped mat under many flying goose\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "string = 'the cat is sitting with the bats on the striped mat under many flying geese'\n",
    "  \n",
    "# Converting String into tokens\n",
    "list2 = nltk.word_tokenize(string)\n",
    "print(list2)\n",
    "#> ['the', 'cat', 'is', 'sitting', 'with', 'the', 'bats', 'on',\n",
    "#   'the', 'striped', 'mat', 'under', 'many', 'flying', 'geese']\n",
    "  \n",
    "lemmatized_string = ' '.join([wnl.lemmatize(words) for words in list2])\n",
    "  \n",
    "print(lemmatized_string)   \n",
    "#> the cat is sitting with the bat on the striped mat under many flying goose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Wordnet Lemmatizer (with POS tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above approach, we observed that Wordnet results were not up to the mark. Words like ‘sitting’, ‘flying’ etc remained the same after lemmatization. This is because these words are treated as a noun in the given sentence rather than a verb. To overcome come this, we use POS (Part of Speech) tags. \n",
    "\n",
    "We add a tag with a particular word defining its type (verb, noun, adjective etc). \n",
    "For Example,\n",
    "\n",
    "Word      +    Type (POS tag)     —>     Lemmatized Word\n",
    "\n",
    "driving    +    verb      ‘v’            —>     drive\n",
    "\n",
    "dogs       +    noun      ‘n’           —>     dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation with lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming or lemmatization?\n",
    "\n",
    "\n",
    "After going through the entire tutorial, you may be asking yourself when should I use Stemming and when should I use Lemmatization? The answer itself is in whatever you have learned from this tutorial. You have seen the following points:\n",
    "\n",
    "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word.\n",
    "\n",
    "Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming. You also had to define a parts-of-speech to obtain the correct lemma.\n",
    "\n",
    "So when to use what! The above p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "the bat saw the cat with stripe hanging upside down by their foot\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob, Word\n",
    "  \n",
    "my_word = 'cats'\n",
    "  \n",
    "# create a Word object\n",
    "w = Word(my_word)\n",
    "  \n",
    "print(w.lemmatize())\n",
    "#> cat\n",
    "  \n",
    "sentence = 'the bats saw the cats with stripes hanging upside down by their feet.'\n",
    "  \n",
    "s = TextBlob(sentence)\n",
    "lemmatized_sentence = \" \".join([w.lemmatize() for w in s.words])\n",
    "  \n",
    "print(lemmatized_sentence)\n",
    "#> the bat saw the cat with stripe hanging upside down by their foot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are spacy, Tree tagger, pattern, Gensim, Stanford CoreNLP lemmatizers\n",
    "\n",
    "https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' stop words  are the words that are filtered before  and after processing of text.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\" STOP WORDS  are the words that are Filtered before  and after processing of text.\"\"\"\n",
    "\n",
    "text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('My', 'Name')\n",
      "('Name', 'is')\n",
      "('is', 'Chaitanya')\n",
      "('Chaitanya', 'Kumar.')\n",
      "('Kumar.', 'I')\n",
      "('I', 'Live')\n",
      "('Live', 'in')\n",
      "('in', 'Totanto.')\n",
      "('Totanto.', 'I')\n",
      "('I', 'work')\n",
      "('work', 'at')\n",
      "('at', 'Microsoft.')\n"
     ]
    }
   ],
   "source": [
    "#Load the package for ngrams\n",
    "from nltk import ngrams\n",
    "#define user input\n",
    "usr_input =  \"\"\"My Name is Chaitanya Kumar. I Live in Totanto. I work at Microsoft.\"\"\"\n",
    "\n",
    "#define number of gram\n",
    "n = 2\n",
    "\n",
    "#split the sentence to make grams\n",
    "sixgrams = ngrams(usr_input.split(), n)\n",
    "\n",
    "\n",
    "for grams in sixgrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Vectorization:  Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 1), (5, 1), (6, 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load Gensim\n",
    "from gensim import corpora\n",
    "\n",
    "#documents for building vocabulary\n",
    "documents = [\"Simplilearn is an ed-tech company\",  \"We provide multiple e-learning courses\"]\n",
    "\n",
    "#text processing\n",
    "texts = [[word for word in document.lower().split()] for document in documents ]\n",
    "\n",
    "#convert into dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "#document to convert in vector\n",
    "new_doc = \"ed-tech company for e-learning courses\"\n",
    "\n",
    "#document to bag of words conversion\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim: Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.050000038), (1, 0.050000038), (2, 0.05000004), (3, 0.5499914), (4, 0.050000038), (5, 0.05000004), (6, 0.050008226), (7, 0.05000004), (8, 0.05000004), (9, 0.05000004)]\n"
     ]
    }
   ],
   "source": [
    "#Loading gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "#create a corpus from a list of text\n",
    "common_dictionary = Dictionary(common_texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n",
    "\n",
    "#Train the model\n",
    "lda = LdaModel(common_corpus, num_topics=10)\n",
    "\n",
    "#new corpus of unseen documents\n",
    "other_texts = [\n",
    " ['data', 'unstructured', 'time'],\n",
    " ['bigdata', 'intelligence', 'natural'],\n",
    " ['language', 'machine', 'computer']\n",
    "]\n",
    "\n",
    "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "\n",
    "unseen_doc = other_corpus[0]\n",
    "#get topic probability distribution for a document\n",
    "vector = lda[unseen_doc]\n",
    "print(vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

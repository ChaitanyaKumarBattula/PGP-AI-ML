{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COURSE:   PGP [AI&ML]\n",
    "\n",
    "## Learner :  Chaitanya Kumar Battula\n",
    "## Module  : RNN\n",
    "## Topic   :  Train the agent to learn and win the card game, Blackjack.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ylMuCnna1QhM"
   },
   "source": [
    "## **Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "11t5mcJF1Sc0"
   },
   "source": [
    "* Game is played against a fixed dealer.\n",
    "* Game has a replacement or an infinite deck.\n",
    "* Moves:\n",
    "  * Hit = Player asking for additional card\n",
    "  * Stick = Player stops asking for the additional card\n",
    "  * Bust = The sum of all cards exceeds 21 \n",
    "* Score of the cards:\n",
    "  * Each of the cards Jack, Queen, and King has a reward of 10.\n",
    "  * Each Ace has a reward of 11 or 1 and is called unstable at 11.\n",
    "* Goal: Acquire cards that add upto 21 and must not go beyond 21.\n",
    "* Rules:\n",
    "  * Game starts with one card faced up and one card faced down for the player and the dealer.\n",
    "  * Player can ask for additional cards until the sum of the cards exceed 21 or player stops voluntarily..\n",
    "  * After the player sticks, the dealer shows the facedown card and draws cards from the deck until the sum is 17 or greater.\n",
    "  * After drawing cards, the player wins if the dealer exceeds the allowed sum of 21 and vice versa.\n",
    "  * If neither of them busts, the winner is decided by finding whoever has a score nearer to 21 \n",
    "* Action:\n",
    "  * STICK = 0\n",
    "  * HIT = 1\n",
    "* Reward:\n",
    "  * Win = +1\n",
    "  * Draw = 0\n",
    "  * Loss = -1\n",
    "* Observation:\n",
    "  * Current sum of players\n",
    "  * Dealer's one showing card\n",
    "  * Player having a usable ace or not\n",
    "\n",
    "Environment courtsey: This environment corresponds to the version of the Blackjack problem described in Example 5.1 in Reinforcement Learning: An Introduction by Sutton and Barto (1998), and OpenAI Gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "monlIs_P1mK8"
   },
   "source": [
    "## **Import Libraries and Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YGMRFEtL1KN_"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import gym\n",
    "env = gym.make(\"Blackjack-v0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fw6JCl681y2v"
   },
   "source": [
    "## **Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pzBl4nVx6rrK"
   },
   "source": [
    "**Arguments:**\n",
    "\n",
    "* policy: Maps an observation to action probabilities\n",
    "* env: OpenAI Gym environment\n",
    "* num_episodes: Number of episodes\n",
    "* discount_factor: Gamma discount factor\n",
    "* Q: A dictionary that maps from state -> action-values. Each value is a numpy array of length nA (see below)\n",
    "* epsilon: Probability to select a random action float between 0 and 1\n",
    "* nA: Number of actions in the environment\n",
    "* Returns:\n",
    "  * A = Function that takes the observation as an argument and returns the probabilities for each action in the form of a numpy array of length nA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = [0.95, 0.05]\n",
    "np.random.choice(np.arange(len(probs)), p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_action = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05, 0.95])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[best_action] += (1.0 - .1)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbhCSKmD1KOO"
   },
   "source": [
    "### **Monte Carlo Control**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zISW8nu-1KOP"
   },
   "outputs": [],
   "source": [
    "#Creating epsilon greedy policy for Q-function and epsilon\n",
    "\n",
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_ohVf1t8EmY"
   },
   "source": [
    "**Arguments:**\n",
    "* num_episodes = Number of episodes as sample\n",
    "* discount_factor = Gamma discount factor\n",
    "* Returns:\n",
    "  * A = Tuple of Q and policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KxzguI-W1KOT"
   },
   "outputs": [],
   "source": [
    "#Finding an optimal epsiolon-greedy policy\n",
    "\n",
    "def mc_control_epsilon_greedy(env, num_episodes, discount_factor=1.0, epsilon=0.1):\n",
    "    \n",
    "    # Keeps track of sum and count of returns for each state\n",
    "    # to calculate an average. We could use an array to save all\n",
    "    # returns (like in the book) but that's memory inefficient.\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # The final action-value function (Q).\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "   \n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # Generate an episode.\n",
    "        # An episode is an array of (state, action, reward) tuples\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        for t in range(100):\n",
    "            probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        # Find all (state, action) pairs we've visited in this episode\n",
    "        # We convert each state to a tuple so that we can use it as a dict key\n",
    "        sa_in_episode = set([(tuple(x[0]), x[1]) for x in episode])\n",
    "        for state, action in sa_in_episode:\n",
    "            sa_pair = (state, action)\n",
    "            # Find the first occurance of the (state, action) pair in the episode\n",
    "            first_occurence_idx = next(i for i,x in enumerate(episode)\n",
    "                                       if x[0] == state and x[1] == action)\n",
    "            # Sum up all rewards since the first occurance\n",
    "            G = sum([x[2]*(discount_factor**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
    "            # Calculate average return for this state over all sampled episodes\n",
    "            returns_sum[sa_pair] += G\n",
    "            returns_count[sa_pair] += 1.0\n",
    "            Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]\n",
    "        \n",
    "        # The policy is improved implicitly by changing the Q dictionary\n",
    "    \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35weqfQk2jIi"
   },
   "source": [
    "#### **Episodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9961,
     "status": "ok",
     "timestamp": 1591866337973,
     "user": {
      "displayName": "Ajay Pal Singh",
      "photoUrl": "",
      "userId": "06233761347829400527"
     },
     "user_tz": -330
    },
    "id": "35d3c8pe1KOV",
    "outputId": "a9556191-0480-4b4a-fda9-c26cba86cfe1"
   },
   "outputs": [],
   "source": [
    "Q, policy = mc_control_epsilon_greedy(env, num_episodes=50, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.mc_control_epsilon_greedy.<locals>.<lambda>()>,\n",
       "            {(18, 10, False): array([ 0., -1.]),\n",
       "             (21, 10, True): array([1., 0.]),\n",
       "             (20, 10, False): array([1., 0.]),\n",
       "             (21, 2, True): array([0., 0.]),\n",
       "             (17, 5, False): array([1., 0.]),\n",
       "             (12, 6, False): array([-1.,  0.]),\n",
       "             (20, 3, False): array([0., 0.]),\n",
       "             (7, 7, False): array([-1.,  0.]),\n",
       "             (9, 10, False): array([-1.,  0.]),\n",
       "             (16, 7, True): array([1., 0.]),\n",
       "             (17, 10, True): array([-1.,  0.]),\n",
       "             (14, 10, False): array([ 0., -1.]),\n",
       "             (20, 9, False): array([ 0., -1.]),\n",
       "             (20, 5, True): array([1., 0.]),\n",
       "             (6, 8, False): array([1., 0.]),\n",
       "             (10, 1, False): array([-1.,  0.]),\n",
       "             (20, 5, False): array([1., 0.]),\n",
       "             (8, 7, False): array([1., 0.]),\n",
       "             (14, 4, False): array([-1.,  0.]),\n",
       "             (12, 9, False): array([-1.,  0.]),\n",
       "             (9, 4, False): array([1., 0.]),\n",
       "             (16, 6, False): array([-1.,  0.]),\n",
       "             (16, 5, True): array([1., 0.]),\n",
       "             (13, 8, False): array([-1.,  0.]),\n",
       "             (16, 8, False): array([-1.,  0.]),\n",
       "             (17, 10, False): array([-1., -1.]),\n",
       "             (15, 2, False): array([-1.,  0.]),\n",
       "             (11, 5, False): array([-1.,  0.]),\n",
       "             (10, 3, False): array([-1.,  0.]),\n",
       "             (13, 10, False): array([-1., -1.]),\n",
       "             (11, 10, False): array([-1.,  1.]),\n",
       "             (15, 8, False): array([-1.,  0.]),\n",
       "             (19, 10, False): array([-1.,  0.]),\n",
       "             (18, 8, False): array([1., 0.]),\n",
       "             (15, 7, False): array([-1.,  0.]),\n",
       "             (14, 2, False): array([-1.,  0.]),\n",
       "             (21, 10, False): array([1., 0.]),\n",
       "             (5, 6, False): array([1., 0.]),\n",
       "             (19, 9, False): array([0., 0.]),\n",
       "             (17, 1, False): array([-1.,  0.]),\n",
       "             (16, 2, False): array([-1.,  0.]),\n",
       "             (11, 4, False): array([-1.,  0.]),\n",
       "             (10, 10, False): array([0., 1.]),\n",
       "             (13, 7, False): array([-1.,  0.]),\n",
       "             (13, 3, False): array([-1.,  0.]),\n",
       "             (18, 6, True): array([-1.,  0.])})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Demo_5_MC_Control.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

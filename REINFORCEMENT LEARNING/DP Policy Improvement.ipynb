{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COURSE:   PGP [AI&ML]\n",
    "\n",
    "## Learner :  Chaitanya Kumar Battula\n",
    "## Module  : RNN\n",
    "## Topic   : Policy Improvement in GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLWRKYoEEDV0"
   },
   "source": [
    "## **Problem Statement:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RfcJfykDIyiD"
   },
   "source": [
    "Company Robo.ai is building a robot that can traverse unassisted, through the environment, and reach the food counter. Instead of creating their own environment, they have planned to use a prebuilt 4x4 grid world. You are a researcher who has to identify the policy and value iteration methods to tackle this task. You have decided to go with the policy iteration method.\n",
    "You have already performed the first step to get new policy. Now, improve the new policy using policy improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQFsPa_-LUm-"
   },
   "source": [
    "## **Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNRT5rmREJfl"
   },
   "source": [
    "This environment possesses two terminal states present at:<br>\n",
    "* Top left corner\n",
    "* Bottom right corner\n",
    "\n",
    "<br>\n",
    "The 4x4 grid looks as follows:<br>\n",
    "T  o  o  o<br>\n",
    "o  x  o  o<br>\n",
    "o  o  o  o<br>\n",
    "o  o  o  T<br>\n",
    "Where x is the position of the agent and T are the two terminal states.<br>\n",
    "\n",
    "<br>\n",
    "The allowed actions are as follows:\n",
    "* UP = 0 \n",
    "* RIGHT = 1 \n",
    "* DOWN = 2 \n",
    "* LEFT = 3 <br>\n",
    "\n",
    "\n",
    "    Note: The agent will move back to current states if it performs an action that leads it to go off the edge.\n",
    "\n",
    "Rewards:\n",
    "The agent is granted a reward of -1 at each step until it reaches a terminal state.\n",
    "\n",
    "Environment courtesy: Sutton's Reinforcement Learning book, chapter 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KzfC5lALiH7"
   },
   "source": [
    "### **Dependencies**\n",
    "* Discrete\n",
    "* Gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1kFapkOLoej"
   },
   "source": [
    "    Note: The steps for policy evaluation are present in this document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krb3JtvVLYF9"
   },
   "source": [
    "## **Import libraries and environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oE9AfY72CI5E"
   },
   "outputs": [],
   "source": [
    "import numpy as nump\n",
    "import sys\n",
    "from gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rS5CVTSwCI5J"
   },
   "outputs": [],
   "source": [
    "environment = GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mfyylf5PLyLH"
   },
   "source": [
    "## **Evaluate the policy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOCx4_o1P3wq"
   },
   "source": [
    "Arguments:\n",
    "    \n",
    "* policy = [S, A] shaped matrix\n",
    "* environment.P = Transition probabilities\n",
    "* environment.P[s][a] = Transition tuple (prob, next_state, reward, done)\n",
    "* environment.nS = Number of states \n",
    "* environment.nA = Number of actions\n",
    "* theta = Stopping the evaluation once the value function changes is less than theta for all the states\n",
    "* discount_factor = Gamma discount factor\n",
    "* Returns = Value function in form of a vector of length environment.nS\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adqFxE_wcuzy"
   },
   "outputs": [],
   "source": [
    "def policy_eval(policy, environment, discount_factor=1.0, theta=0.00001):\n",
    "    \n",
    "    # Start with a random value function where the value is 0 for all the states.\n",
    "    Val_function = nump.zeros(environment.nS)\n",
    "    while True:\n",
    "      \n",
    "        delta = 0\n",
    "        # Perform a \"full backup\" for each state\n",
    "        for s in range(environment.nS):\n",
    "            v = 0\n",
    "            \n",
    "            # Look at all the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "              \n",
    "                # Look at the possible next states in accordance to all the 4 types of actions\n",
    "                for  prob, next_state, reward, done in environment.P[s][a]:\n",
    "                  \n",
    "                    # Calculate the expected value\n",
    "                    v += action_prob * prob * (reward + discount_factor * Val_function[next_state])\n",
    "                    \n",
    "            # Register the change in value function across any state\n",
    "            delta = max(delta, nump.abs(v - Val_function[s]))\n",
    "            Val_function[s] = v\n",
    "              \n",
    "        # Cease the evaluation once the value function change is below a threshold i.e, theta\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return nump.array(Val_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5a4ht3POZfer"
   },
   "source": [
    "## **Improve the Policy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TwJa_13viAfK"
   },
   "source": [
    "Arguments:\n",
    "\n",
    "* policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "  * policy\n",
    "  * environment\n",
    "  * discount_factor\n",
    "* Returns: It is a tuple of policy, Val_function \n",
    "* Returns under one-step lookahead: It is a vector of length environment.nA<br> that contains the expected value of each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Gp9TU8iZire"
   },
   "outputs": [],
   "source": [
    "#Borrowing the evaluated policy from policy evaluation\n",
    "def policy_improvement(environment, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "\n",
    "    #Defining one step lookahead to find the value function\n",
    "    def one_step_lookahead(state, Val_function):\n",
    "        \n",
    "        A = nump.zeros(environment.nA)\n",
    "        for a in range(environment.nA):\n",
    "            for prob, next_state, reward, done in environment.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * Val_function[next_state])\n",
    "        return A\n",
    "      \n",
    "    # Start with a random policy\n",
    "    policy = nump.ones([environment.nS, environment.nA]) / environment.nA\n",
    "    \n",
    "    while True:\n",
    "        # Evaluate the current policy\n",
    "        Val_function = policy_eval_fn(policy, environment, discount_factor)\n",
    "        \n",
    "        # Any changes to the policy will set it to False:\n",
    "        policy_stable = True\n",
    "        \n",
    "        # For each state\n",
    "        for s in range(environment.nS):\n",
    "            # The best action taken under the currect policy\n",
    "            chosen_a = nump.argmax(policy[s])\n",
    "            \n",
    "            # One-step lookahead finds the best action \n",
    "            # Arbitarily resolving the ties\n",
    "            action_values = one_step_lookahead(s, Val_function)\n",
    "            best_a = nump.argmax(action_values)\n",
    "            \n",
    "            # Greedy update of the policy\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = nump.eye(environment.nA)[best_a]\n",
    "        \n",
    "        # Return the best stable optimal policy\n",
    "        if policy_stable:\n",
    "            return policy, Val_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1164,
     "status": "ok",
     "timestamp": 1593790796372,
     "user": {
      "displayName": "Ajay Pal Singh",
      "photoUrl": "",
      "userId": "06233761347829400527"
     },
     "user_tz": -330
    },
    "id": "_YOJvJHca5Va",
    "outputId": "209ca234-a305-4e91-9377-b2a16680fac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "\n",
      "Value Function:\n",
      "[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy, v = policy_improvement(environment)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(nump.reshape(nump.argmax(policy, axis=1), environment.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(environment.shape))\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Demo_2_DP_Policy_Improvement.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
